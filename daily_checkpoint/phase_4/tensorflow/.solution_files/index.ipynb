{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Neural Network Regularization\n", "\n", "This assessment covers building and training a `tf.keras` `Sequential` model, then applying regularization.  The dataset comes from a [\"don't overfit\" Kaggle competition](https://www.kaggle.com/c/dont-overfit-ii).  There are 300 features labeled 0-299, and a target called \"target\".  There are only 250 records total, meaning this is a very small dataset to be used with a neural network. \n", "\n", "_You can assume that the dataset has already been scaled._"]}, {"cell_type": "code", "execution_count": 1, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import pandas as pd\n", "from sklearn.metrics import accuracy_score\n", "from sklearn.model_selection import train_test_split, cross_val_score\n", "\n", "import tensorflow as tf\n", "from tensorflow.keras import Sequential, regularizers\n", "from tensorflow.keras.layers import Dense, Dropout\n", "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n", "tf.logging.set_verbosity(tf.logging.ERROR)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the cells below, the set of data has been split into a training and testing set and then fit to a neural network with two hidden layers. Run the cells below to see how well the model performs."]}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [{"data": {"text/plain": ["(187, 300)"]}, "execution_count": 2, "metadata": {}, "output_type": "execute_result"}], "source": ["df = pd.read_csv(\"data.csv\")\n", "df.drop(\"id\", axis=1, inplace=True)\n", "\n", "X = df.drop(\"target\", axis=1)\n", "y = df[\"target\"]\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2020)\n", "X_train.shape"]}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": ["\n", "def build_model():\n", "    classifier = Sequential()\n", "    classifier.add(Dense(units=64, input_shape=(300,)))\n", "    classifier.add(Dense(units=64))\n", "    classifier.add(Dense(units=64))\n", "    classifier.add(Dense(units=1, activation='sigmoid'))\n", "    classifier.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n", "    return classifier"]}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": ["def fit_and_cross_validate_model(model_func, X, y):\n", "    \"\"\"\n", "    Given a function that builds a model and training X and y, validate the model based on\n", "    cross-validated train and test data\n", "    \"\"\"\n", "    keras_classifier = KerasClassifier(build_model, epochs=5, batch_size=50, verbose=1, shuffle=False)\n", "    print(\"######################## Training cross-validated models ###########################\")\n", "    cross_val_scores = cross_val_score(keras_classifier, X, y, cv=5)\n", "    print(\"########################### Training on full X_train ###############################\")\n", "    keras_classifier.fit(X, y)\n", "    print(\"############################### Evaluation report ##################################\")\n", "    print(\"Approximate training accuracy:\")\n", "    print(accuracy_score(y, keras_classifier.predict(X)))\n", "    print(\"Approximate testing accuracy:\")\n", "    print(np.mean(cross_val_scores), \"+/-\", np.std(cross_val_scores))"]}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["######################## Training cross-validated models ###########################\n", "Epoch 1/5\n", "149/149 [==============================] - 0s 2ms/sample - loss: 1.0130 - acc: 0.4698\n", "Epoch 2/5\n", "149/149 [==============================] - 0s 109us/sample - loss: 0.6362 - acc: 0.6242\n", "Epoch 3/5\n", "149/149 [==============================] - 0s 124us/sample - loss: 0.4664 - acc: 0.7987\n", "Epoch 4/5\n", "149/149 [==============================] - 0s 91us/sample - loss: 0.3688 - acc: 0.8523\n", "Epoch 5/5\n", "149/149 [==============================] - 0s 130us/sample - loss: 0.2980 - acc: 0.8792\n", "38/38 [==============================] - 0s 2ms/sample - loss: 0.8184 - acc: 0.5789\n", "Epoch 1/5\n", "149/149 [==============================] - 0s 2ms/sample - loss: 0.8932 - acc: 0.4899\n", "Epoch 2/5\n", "149/149 [==============================] - 0s 133us/sample - loss: 0.5847 - acc: 0.6913\n", "Epoch 3/5\n", "149/149 [==============================] - 0s 127us/sample - loss: 0.4323 - acc: 0.8322\n", "Epoch 4/5\n", "149/149 [==============================] - 0s 111us/sample - loss: 0.3329 - acc: 0.8859\n", "Epoch 5/5\n", "149/149 [==============================] - 0s 88us/sample - loss: 0.2548 - acc: 0.9329\n", "38/38 [==============================] - 0s 2ms/sample - loss: 1.0012 - acc: 0.6579\n", "Epoch 1/5\n", "150/150 [==============================] - 0s 2ms/sample - loss: 1.0751 - acc: 0.4800\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 105us/sample - loss: 0.6667 - acc: 0.6467\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 122us/sample - loss: 0.4763 - acc: 0.7733\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 104us/sample - loss: 0.3695 - acc: 0.8800\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 78us/sample - loss: 0.2930 - acc: 0.9000\n", "37/37 [==============================] - 0s 2ms/sample - loss: 0.8841 - acc: 0.5946\n", "Epoch 1/5\n", "150/150 [==============================] - 0s 2ms/sample - loss: 0.8584 - acc: 0.5600\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 108us/sample - loss: 0.5741 - acc: 0.7133\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 121us/sample - loss: 0.4419 - acc: 0.8333\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 154us/sample - loss: 0.3571 - acc: 0.9133\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 152us/sample - loss: 0.2861 - acc: 0.9333\n", "37/37 [==============================] - 0s 3ms/sample - loss: 0.6768 - acc: 0.6757\n", "Epoch 1/5\n", "150/150 [==============================] - 0s 2ms/sample - loss: 1.1179 - acc: 0.4467\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 127us/sample - loss: 0.6902 - acc: 0.6000\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 172us/sample - loss: 0.4900 - acc: 0.7200\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 154us/sample - loss: 0.3694 - acc: 0.8333\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 133us/sample - loss: 0.2806 - acc: 0.9333\n", "37/37 [==============================] - 0s 4ms/sample - loss: 0.9008 - acc: 0.5135\n", "########################### Training on full X_train ###############################\n", "Epoch 1/5\n", "187/187 [==============================] - 0s 2ms/sample - loss: 1.1288 - acc: 0.4545\n", "Epoch 2/5\n", "187/187 [==============================] - 0s 113us/sample - loss: 0.6296 - acc: 0.6417\n", "Epoch 3/5\n", "187/187 [==============================] - 0s 98us/sample - loss: 0.4577 - acc: 0.8128\n", "Epoch 4/5\n", "187/187 [==============================] - 0s 202us/sample - loss: 0.3693 - acc: 0.8610\n", "Epoch 5/5\n", "187/187 [==============================] - 0s 108us/sample - loss: 0.2969 - acc: 0.8877\n", "############################### Evaluation report ##################################\n", "Approximate training accuracy:\n", "187/187 [==============================] - 0s 456us/sample\n", "0.9411764705882353\n", "Approximate testing accuracy:\n", "0.60412517786026 +/- 0.05821661272907996\n"]}], "source": ["fit_and_cross_validate_model(build_model, X_train, y_train);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1) Modify the code below to use regularization\n", "\n", "\n", "The model appears to be overfitting. To deal with this overfitting, modify the code below to include regularization in the model. You can add L1, L2, both L1 and L2, or dropout regularization."]}, {"cell_type": "markdown", "metadata": {}, "source": ["Hint: these might be helpful\n", "\n", " - [`Dense` layer documentation](https://keras.io/layers/core/)\n", " - [`regularizers` documentation](https://keras.io/regularizers/)"]}, {"cell_type": "code", "execution_count": 6, "metadata": {}, "outputs": [], "source": ["def build_model_with_regularization():\n", "    classifier = Sequential()\n", "    classifier.add(Dense(units=64, input_shape=(300,), kernel_regularizer=regularizers.l2(0.0000000000000001)))\n", "    # they might add a kernel regularizer\n", "    classifier.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.0000000000000001)))\n", "    # they might add a dropout layer\n", "    classifier.add(Dropout(0.8))\n", "    classifier.add(Dense(units=64, kernel_regularizer=regularizers.l2(0.0000000000000001)))\n", "    classifier.add(Dense(units=1, activation='sigmoid'))\n", "    classifier.compile(optimizer='adam', loss=\"binary_crossentropy\", metrics=['accuracy'])\n", "    return classifier\n"]}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["######################## Training cross-validated models ###########################\n", "Epoch 1/5\n", "149/149 [==============================] - 0s 2ms/sample - loss: 0.8535 - acc: 0.5302\n", "Epoch 2/5\n", "149/149 [==============================] - 0s 121us/sample - loss: 0.5429 - acc: 0.7248\n", "Epoch 3/5\n", "149/149 [==============================] - 0s 327us/sample - loss: 0.3991 - acc: 0.8456\n", "Epoch 4/5\n", "149/149 [==============================] - 0s 262us/sample - loss: 0.3093 - acc: 0.8658\n", "Epoch 5/5\n", "149/149 [==============================] - 0s 122us/sample - loss: 0.2406 - acc: 0.9128\n", "38/38 [==============================] - 0s 4ms/sample - loss: 0.8746 - acc: 0.5789\n", "Epoch 1/5\n", "149/149 [==============================] - 0s 3ms/sample - loss: 0.9684 - acc: 0.5101\n", "Epoch 2/5\n", "149/149 [==============================] - 0s 156us/sample - loss: 0.5478 - acc: 0.6913\n", "Epoch 3/5\n", "149/149 [==============================] - 0s 128us/sample - loss: 0.3774 - acc: 0.8523\n", "Epoch 4/5\n", "149/149 [==============================] - 0s 139us/sample - loss: 0.2825 - acc: 0.8993\n", "Epoch 5/5\n", "149/149 [==============================] - 0s 163us/sample - loss: 0.2096 - acc: 0.9463\n", "38/38 [==============================] - 0s 4ms/sample - loss: 0.9508 - acc: 0.5526\n", "Epoch 1/5\n", "150/150 [==============================] - 0s 3ms/sample - loss: 0.7792 - acc: 0.5600\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 129us/sample - loss: 0.5105 - acc: 0.7400\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 120us/sample - loss: 0.3802 - acc: 0.8533\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 166us/sample - loss: 0.2944 - acc: 0.9200\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 115us/sample - loss: 0.2266 - acc: 0.9467\n", "37/37 [==============================] - 0s 4ms/sample - loss: 1.0143 - acc: 0.5135\n", "Epoch 1/5\n", "150/150 [==============================] - 0s 3ms/sample - loss: 0.9874 - acc: 0.4533\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 96us/sample - loss: 0.6195 - acc: 0.7000\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 117us/sample - loss: 0.4555 - acc: 0.7933\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 124us/sample - loss: 0.3512 - acc: 0.8667\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 97us/sample - loss: 0.2652 - acc: 0.9533\n", "37/37 [==============================] - 0s 5ms/sample - loss: 0.7550 - acc: 0.6216\n", "Epoch 1/5\n", "150/150 [==============================] - 1s 4ms/sample - loss: 1.0522 - acc: 0.5067\n", "Epoch 2/5\n", "150/150 [==============================] - 0s 158us/sample - loss: 0.6428 - acc: 0.6800\n", "Epoch 3/5\n", "150/150 [==============================] - 0s 231us/sample - loss: 0.4589 - acc: 0.7933\n", "Epoch 4/5\n", "150/150 [==============================] - 0s 100us/sample - loss: 0.3493 - acc: 0.8800\n", "Epoch 5/5\n", "150/150 [==============================] - 0s 119us/sample - loss: 0.2693 - acc: 0.9400\n", "37/37 [==============================] - 0s 6ms/sample - loss: 0.9054 - acc: 0.5135\n", "########################### Training on full X_train ###############################\n", "Epoch 1/5\n", "187/187 [==============================] - 0s 3ms/sample - loss: 0.9576 - acc: 0.4599\n", "Epoch 2/5\n", "187/187 [==============================] - 0s 153us/sample - loss: 0.6224 - acc: 0.6417\n", "Epoch 3/5\n", "187/187 [==============================] - 0s 172us/sample - loss: 0.4819 - acc: 0.8021\n", "Epoch 4/5\n", "187/187 [==============================] - 0s 259us/sample - loss: 0.3878 - acc: 0.8556\n", "Epoch 5/5\n", "187/187 [==============================] - 0s 107us/sample - loss: 0.3064 - acc: 0.8930\n", "############################### Evaluation report ##################################\n", "Approximate training accuracy:\n", "187/187 [==============================] - 0s 913us/sample\n", "0.9144385026737968\n", "Approximate testing accuracy:\n", "0.5560455083847046 +/- 0.041120110886002884\n"]}], "source": ["fit_and_cross_validate_model(build_model_with_regularization, X_train, y_train);"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Based on the cross-validated scores, did the regularization you performed help prevent overfitting? Is the first or the second model better?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# It may or may not have prevented overfitting, depending on random elements\n", "# within the neural net as well as their choice of regularization technique\n", "#\n", "# (TensorFlow + random seeding is not fully possible in a Jupyter Notebook)\n", "#\n", "# The student should interpret the numbers they have\n", "#\n", "# In the example given above, a reasonable answer would be:\n", "# The regularization is helping to prevent overfitting, but it also might be\n", "# causing some underfitting.  The train and test accuracy are more similar to\n", "# each other, but the test accuracy also got slightly worse.  I think the\n", "# original model is better, even though it is overfitting.\n", "#\n", "# It is also very likely that they will not have applied strong enough\n", "# regularization to make a difference, so the scores for the two models will\n", "# mainly differ based on random seeds"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Now, evaluate both models on the holdout set"]}, {"cell_type": "code", "execution_count": 8, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": ["Epoch 1/5\n", "187/187 [==============================] - 0s 2ms/sample - loss: 1.0741 - acc: 0.4652\n", "Epoch 2/5\n", "187/187 [==============================] - 0s 145us/sample - loss: 0.6118 - acc: 0.7112\n", "Epoch 3/5\n", "187/187 [==============================] - 0s 93us/sample - loss: 0.4258 - acc: 0.8449\n", "Epoch 4/5\n", "187/187 [==============================] - 0s 105us/sample - loss: 0.3201 - acc: 0.9037\n", "Epoch 5/5\n", "187/187 [==============================] - 0s 142us/sample - loss: 0.2375 - acc: 0.9412\n", "Epoch 1/5\n", "187/187 [==============================] - 1s 3ms/sample - loss: 1.9396 - acc: 0.4492\n", "Epoch 2/5\n", "187/187 [==============================] - 0s 177us/sample - loss: 1.4191 - acc: 0.5080\n", "Epoch 3/5\n", "187/187 [==============================] - 0s 167us/sample - loss: 1.1469 - acc: 0.5455\n", "Epoch 4/5\n", "187/187 [==============================] - 0s 128us/sample - loss: 0.9037 - acc: 0.6417\n", "Epoch 5/5\n", "187/187 [==============================] - 0s 86us/sample - loss: 0.8540 - acc: 0.5775\n", "Accuracy score without regularization: 0.6031746031746031\n", "Accuracy score with regularization: 0.5396825396825397\n"]}], "source": ["classifier_1 = build_model()\n", "classifier_1.fit(X_train, y_train, epochs=5, verbose=1, batch_size=50, shuffle=False)\n", "\n", "classifier_2 = build_model_with_regularization()\n", "classifier_2.fit(X_train, y_train, epochs=5, verbose=1, batch_size=50, shuffle=False)\n", "\n", "print(\"Accuracy score without regularization:\", accuracy_score(y_test, classifier_1.predict_classes(X_test)))\n", "print(\"Accuracy score with regularization:\", accuracy_score(y_test, classifier_2.predict_classes(X_test)))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2) Explain how regularization is related to the bias/variance tradeoff within Neural Networks and how it's related to the results you just achieved in the training and test accuracies of the previous models. What does regularization change in the training process (be specific to what is being regularized and how it is regularizing)?\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Regularization helps prevent over fitting by adding penalty terms to the cost function. \n", "# This prevents any one feature to having too much importance in a model.  One feature\n", "# having too much importance can lead to overfitting (high variance).  On the other hand,\n", "# too much regularization can lead to underfitting (high bias).\n", "#\n", "# The specific regularization used in the solution code is:\n", "# L2 regularization: penalizes weight matrices for being too large\n", "# Dropout regularization: a random subset of nodes are ignored\n", "#\n", "# The current dataset is very small to be used with a neural network, so it's possible that\n", "# we don't actually have enough information to create a good, generalizable model"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3) How might L1  and dropout regularization change a neural network's architecture?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# L1 and dropout regularization may eliminate connections between nodes entirely."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.7"}}, "nbformat": 4, "nbformat_minor": 4}